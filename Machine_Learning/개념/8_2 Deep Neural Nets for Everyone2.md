#### CIFAR

- Canadian Institute for Advanced Research(CIFAR)
- CIFAR encourages basic research without direct application, was what motivated
Hinton to move to Canada in 1987, and funded his work afterward.

활용도가 높지 않아도 연구를 독려함

#### "Everyone else was doing something different"

- "It was the worst possible time," says Bengio, a professor at the Universite de Montreal and
co-director of the CIFAR program since it was renewed last year.
"Everyone else was doing something different. Somehow, Geoff convinced them."
- "We should give (CIFAR) a lot of credit for making that gamble."
- CIFAR "had a huge impact in forming a community around deep learning," adds LeCun
- In 2006, Hinton, Simon Osindero, and Yee-Whye Teh published, "A fast learning algorithm for deep belief nets"
- Yoshua Bengio et al. in 2007 with "Greedy Layer-Wise Training of Deep Networks"

#### Breakthrough(in 2006 and 2007 by Hinton and Bengio)

layer가 많은 것은 학습할 수 없다고 했었는데 초기값을 잘주면 학습이 가능하다는 것을 밝혀냄

- Neural networks with many layers really could be trained well, if the weights are initialized in a clever way
rather than randomly.
- Deep machine learning methods are more efficient for difficult problems than shallow methods.
- Rebranding to Deep Nets, Deep Learning

#### Neural networks that can explain photos

#### Deep API Learning

자연어로 시스템에게 입력을 주면 시스템이 자동적으로 어떤 API를 어떤 순서로 사용해야하는지 알려줌

#### Geoffrey Hinton's summary of findings up to today

이전에는 잘 돌아가지 않았던 이유를 4가지로 밝힘

1. Our labeled datasets were thousands of times too small.
2. Our computers were millions of times too slow.
3. We initialized the weights in a stupid way.
4. We used the wrong type of non-linearity.

#### Why Now?
1. Students/Researchers
- Not too late to be a world expert
- Not too complicated (mathematically)

2. Practitioner
- Accurate enough to be used in practice
- many ready-to-use tools such as TensorFlow
- Many easy/simple programming languages such as Python

3. After all, it is fun!
